{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions to the exercises set 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Posterior mean of binomial random variable as mixture of prior and likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The posterior is given by:\n",
    "\n",
    "$$p(N, \\mu | m) = \\frac{p(m | N, \\mu) \\cdot p(N, \\mu)}{p(m)}$$\n",
    "\n",
    "Let's look at the form of the dependence of $\\mu$ to parameters of our distribution:\n",
    "\n",
    "$$p(\\mu |N, m) \\propto \\mu^m (1 - \\mu)^{N - m} \\times \\mu^{a - 1} (1 -\\mu)^{b - 1} = \\mu^{m + a - 1} (1 - \\mu)^{N - m + b - 1}$$\n",
    "\n",
    "Posterior is valid distribution and its part that depends on $\\mu$ is the same as for Beta distribution with parameters $\\alpha + m$ and $ \\beta + N - m$. So after normalization the exact posterior is given by: \n",
    "\n",
    "$$p( \\mu | N,m) = Beta(\\alpha + m, \\beta + N - m) $$\n",
    "\n",
    "So using the mean of Beta distribution \n",
    "\n",
    "$$\\mathbb{E}_{p(\\mu |N, m) }[\\mu]= \\frac{\\alpha + m}{\\alpha + m + \\beta + N - m} = \\frac{\\alpha + m}{\\alpha + \\beta + N}$$\n",
    "\n",
    "This can be written as a mixture prior and likelihood mean:\n",
    "\n",
    "$$ \\mathbb{E}_{p(\\mu |N, m) }[\\mu] = \\frac{\\alpha + \\beta}{\\alpha + \\beta + N} \\cdot \\frac{\\alpha}{\\alpha + \\beta} + \\frac{N}{\\alpha + \\beta + n} \\cdot \\frac{m}{N} $$\n",
    "\n",
    "With:\n",
    "\n",
    "\\begin{equation}\n",
    "\\lambda = \\frac{\\alpha + \\beta}{\\alpha + \\beta + N}\n",
    "\\end{equation}\n",
    "\n",
    "we get:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}_{p(\\mu |N, m) }[\\mu] = \\lambda \\cdot \\frac{\\alpha}{\\alpha + \\beta} + (1 - \\lambda) \\cdot \\frac{m}{N}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bivariate Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From multivariate Gaussian:\n",
    "\n",
    "$$ p(\\mathbf{x}) = \\frac{1}{\\sqrt{(2 \\pi)^2 \\det{(\\Sigma)}}} \\exp\\left(-\\frac{1}{2} (\\mathbf{x} - \\mathbf{\\mu})^{T} \\Sigma^{-1} (\\mathbf{x} - \\mathbf{\\mu}) \\right) $$\n",
    "\n",
    "For bivariate Gaussian: \n",
    "$$ \\mathbf{x}=\\begin{bmatrix}\n",
    "    x_1 \\\\ x_2 \\\\\n",
    "\\end{bmatrix}, \\quad \\mathbf{\\mu}=\\begin{bmatrix}\n",
    "    \\mu_1 \\\\ \n",
    "    \\mu_2 \\\\\n",
    "\\end{bmatrix}, \n",
    "\\\\\n",
    "\\det(\\Sigma) = \\sigma_1^2 \\sigma_2^2 (1 - \\rho^2), \\\\\n",
    "\\Sigma^{-1} = \\frac{1}{\\sigma_1^2 \\sigma_2^2 (1 - \\rho^2)} \\begin{pmatrix} \\sigma_2^2 & - \\rho \\sigma_1 \\sigma_2 \\\\\n",
    "- \\rho \\sigma_1 \\sigma_2 & \\sigma_1^2\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "So we can substitute all this in our multivariate Gaussian density:\n",
    "\n",
    "$$\n",
    "p(x_1, x_2) = \\frac{1}{2 \\pi \\sigma_1 \\sigma_2\\sqrt{1- \\rho^2}}\\exp\\left(-\\frac{1}{2} (x - \\mu)^{T} \\Sigma^{-1} (x - \\mu)\\right) = \\\\ \\frac{1}{2 \\pi \\sigma_1 \\sigma_2\\sqrt{1- \\rho^2}}\\exp\\left(-\\frac{1}{2 (\\sigma_1^2 \\sigma_2^2 (1 - \\rho^2))} \\cdot \\left((x_1 - \\mu_1)^2 \\sigma_2^2 + (x_2 - \\mu_2)^2 \\sigma_1^2 - 2 \\rho \\sigma_1 \\sigma_2 (x_1 - \\mu_1) (x_2 - \\mu_2)\\right) \\right)= \\\\ \\frac{1}{2 \\pi \\sigma_1 \\sigma_2\\sqrt{1- \\rho^2}} \\exp\\left(-\\frac{1}{2 (1 - \\rho^2)} \\cdot \\left(\\frac{(x_1 - \\mu_1)^2}{\\sigma_1^2} + \\frac{(x_2 - \\mu_2)^2}{\\sigma_2^2} - 2 \\rho \\frac{(x_1 - \\mu_1)}{\\sigma_1} \\frac{(x_2 - \\mu_2)}{\\sigma_2}\\right) \\right) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E3.  \n",
    "\n",
    "$$ p(x_2 | x_1) = \\frac{p(x_1, x_2)}{p(x_1)} = \\\\ \\frac{\\sqrt{(2 \\pi \\sigma_1^2)}}{2 \\pi \\sigma_1 \\sigma_2\\sqrt{1- \\rho^2}} \\exp\\left(-\\frac{1}{2 (1 - \\rho^2)} \\cdot \\left(\\frac{(x_1 - \\mu_1)^2}{\\sigma_1^2} + \\frac{(x_2 - \\mu_2)^2}{\\sigma_2^2} - 2 \\rho \\frac{(x_1 - \\mu_1)}{\\sigma_1} \\frac{(x_2 - \\mu_2)}{\\sigma_2}\\right) + \\frac{(x_1 - \\mu_1)^2}{2 \\sigma_1^2} \\right) =\\\\\n",
    "= \\frac{1}{\\sqrt{2 \\pi \\sigma_2^2 (1 - \\rho^2)}} \\exp\\left(-\\frac{1}{2 (1 - \\rho^2)} \\cdot \\left(\\frac{\\rho^2 (x_1 - \\mu_1)^2}{\\sigma_1^2} + \\frac{(x_2 - \\mu_2)^2}{\\sigma_2^2} - 2 \\rho \\frac{(x_1 - \\mu_1)}{\\sigma_1} \\frac{(x_2 - \\mu_2)}{\\sigma_2}\\right)\\right)$$\n",
    "\n",
    "Now let's try to find out what is this distribution, it looks like normal so lets show it: \n",
    "$$p(x_2 | x_1) = \\frac{1}{\\sqrt{2 \\pi \\sigma_2^2 (1 - \\rho^2)}} \\exp\\left(-\\frac{1}{2 \\sigma_2^2 (1 - \\rho^2)} \\cdot (x_2 - \\mu_2 - \\frac{\\sigma_2}{\\sigma_1} \\rho (x_1 - \\mu_1))^2\\right) $$\n",
    "\n",
    "Thus, \n",
    "$$ p(x_2 | x_1) \\sim \\mathcal{N}(\\mu_2 + \\frac{\\sigma_2}{\\sigma_1} \\rho (x_1 - \\mu_1),\\sigma_2^2 (1 - \\rho^2)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sensor fusion with 1D Gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior can be calculated via: \n",
    "\n",
    "\\begin{equation}\n",
    "p(\\mu | D) = \\frac{p(D | \\mu) p(\\mu)}{p(D)}\n",
    "\\end{equation}\n",
    "\n",
    "The prior is assumed to be non-informativ. This can be simulated by a Gaussian with zero precision. The likelihood is given by (assume independence):\n",
    "\n",
    "\\begin{equation}\n",
    "p(D | \\mu) = p(D_1, D_2| \\mu) = \\prod_{j = 1}^{N_1} p(y_j|\\mu, \\sigma_1) \\cdot \\prod_{i = 1}^{N_2} p(y_i|\\mu, \\sigma_2)\n",
    "\\end{equation}\n",
    "where  $y_j \\sim \\mathcal{N}(\\mu, \\sigma_1)$ and $y_i \\sim \\mathcal{N}(\\mu, \\sigma_2)$ \n",
    "\n",
    "As the prior is non-informativ and p(D) some value only dependent on D, the prosterior is equal:\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\mu | D) = const(\\mu)\\prod_{i = 1}^{N_1}  \\exp\\left(-\\frac{\\left(y_i^{(1)}-\\mu\\right)^2}{2 \\sigma_1^2}\\right) \\cdot \\prod_{j = 1}^{N_2}  \\exp\\left(-\\frac{\\left(y_j^{(2)}-\\mu\\right)^2}{2 \\sigma_2^2}\\right) =\\\\\n",
    "const(\\mu) \\exp\\left(-\\frac{\\sum_{i = 1}^{N_1} \\left(y_i^{(1)}-\\mu\\right)^2}{2 \\sigma_1^2}-\\frac{\\sum_{j = 1}^{N_2} \\left(y_j^{(2)}-\\mu\\right)^2}{2 \\sigma_2^2} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "It is a Normal distribution. \n",
    "\n",
    "To find mean we can find the $\\mu$ that maximize log of this density:\n",
    "$$\\frac{1}{2\\sigma_1^2}\\sum_{i=1}^{N_1}2\\left(y_{i}^{(1)}-\\mu\\right)+\\frac{1}{2\\sigma_2^2}\\sum_{j=1}^{N_2}2\\left(y_{j}^{(2)}-\\mu\\right)=0$$\n",
    "\n",
    "\\begin{equation}\n",
    "\\mu_{post} = \\left(\\frac{N_1}{\\sigma_1^2} + \\frac{N_2}{\\sigma_2^2}\\right)^{-1} \\left(\\frac{1}{\\sigma_1^2}\\sum_{i = 1}^{N_1} y_i^{(1)} + \\frac{1}{\\sigma_2^2}\\sum_{j_1}^{N_2} y_j^{(2)}\\right)\n",
    "\\end{equation}\n",
    "To find the variance we can calculate the coefficient of $\\mu^2$:\n",
    "$$-\\frac{1}{2\\sigma_{post}^2}=-\\frac{1}{2}\\left(\\frac{N_1}{\\sigma_1^2}+\\frac{N_2}{\\sigma_2^2}\\right)$$\n",
    "$$\\sigma_{post}^2=\\left(\\frac{N_1}{\\sigma_1^2}+\\frac{N_2}{\\sigma_2^2}\\right)^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E5.\n",
    "\n",
    "As we had uninformative prior in previous task, the $\\mu$ that maximize posterior distribution will also maximize our likelihood, so  \n",
    "$$\\mu_{ML}= \\mu_{post} = \\left({N_1}\\lambda_1 +{N_2}\\lambda_2 \\right)^{-1} \\left(\\lambda_1\\sum_{i = 1}^{N_1} y_i^{(1)} + \\lambda_2\\sum_{j_1}^{N_2} y_j^{(2)}\\right)$$\n",
    "Let's write down log likelihood\n",
    "\\begin{equation}\n",
    "LL = \\frac{N_1 }{2}\\log\\left(\\frac{\\lambda_1}{2 \\pi}\\right) - \\frac{\\lambda_1}{2} \\sum_{i = 1}^{N_1}(y_i^{(1)} - \\mu)^2 + \\frac{N_2 }{2}\\log\\left(\\frac{\\lambda_2}{2 \\pi}\\right) - \\frac{\\lambda_2}{2} \\sum_{j = 1}^{N_2}(y_j^{(2)} - \\mu)^2\n",
    "\\end{equation}\n",
    "\n",
    "$$\\frac{\\partial LL}{\\partial \\lambda_1}= \\frac{1}{N_1}\\frac{2\\pi}{\\lambda_1}\\frac{1}{2\\pi} -\\frac{1}{2} \\sum_{i = 1}^{N_1}(y_i^{(1)} - \\mu)^2 =0$$\n",
    "$${\\lambda_1}_{ML}=N_1\\left(\\sum_{i = 1}^{N_1}(y_i^{(1)} - \\mu_{ML})^2 \\right)^{-1}$$\n",
    "$${\\lambda_2}_{ML}=N_2\\left(\\sum_{j = 1}^{N_2}(y_j^{(2)} - \\mu_{ML})^2 \\right)^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11538461538 1.85632688928 0.439143540173\n",
      "1.88261746659 3.26374884013 0.336029831669\n",
      "1.68669471782 5.13202341519 0.274117092759\n",
      "1.60140953324 5.87254601625 0.252229344161\n",
      "1.58236362293 5.99578737002 0.247688492063\n",
      "1.57934314075 6.01339782685 0.246979272406\n",
      "1.57890236275 6.01592115289 0.246876022671\n",
      "1.57883890081 6.01628347016 0.246861162234\n",
      "1.57882978175 6.01633551238 0.246859026994\n",
      "1.57882847178 6.01634298796 0.246858720264\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_1=np.asarray([1.1,1.9])\n",
    "y_2=np.asarray([2.9,4.1])\n",
    "var_1=((y_1-y_1.mean())**2).sum()/1 ; \n",
    "lambda_1 = 1/var_1\n",
    "lambda_2 = 1/var_2\n",
    "var_2=((y_2-y_2.mean())**2).sum()/1\n",
    "\n",
    "for itr in range(10):\n",
    "    mu = (2*lambda_1+2*lambda_2)**(-1)*(lambda_1*y_1.sum()+lambda_2*y_2.sum())\n",
    "    lambda_1 = 2/((y_1-mu)**2).sum()\n",
    "    lambda_2 = 2/((y_2-mu)**2).sum()\n",
    "    print(mu, lambda_1, lambda_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Bayesian linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find this values by looking at the coeficients of multivatiate Gaussian \n",
    "\n",
    "So for \n",
    "$$p(\\mathbf{w}|\\mathbf{t})=\\mathcal{N}(\\mathbf{m_N}, \\mathbf{S_{N}}) \\propto \\exp \\left( \\left(\\mathbf{w}-\\mathbf{m_N}\\right)^{T}\\mathbf{S_{N}}^{-1}\\left(\\mathbf{w}-\\mathbf{m_N}\\right)\\right)= \\\\\n",
    "\\exp \\left( \\mathbf{w}^{T}\\mathbf{S_{N}}^{-1}\\mathbf{w}- 2\\mathbf{w}^{T}\\mathbf{S_{N}}^{-1}\\mathbf{m_N}+const(\\mathbf{w})\\right)\n",
    "$$\n",
    "Now let's write the same exprecion for our posterior \n",
    "$$p(\\mathbf{w}|\\mathbf{t}) \\propto exp \\left( \\left(\\mathbf{w}-\\mathbf{m_0}\\right)^{T}\\mathbf{S_{0}}^{-1}\\left(\\mathbf{w}-\\mathbf{m_0}\\right) +\\left(\\mathbf{t}-\\mathbf{\\Phi  w}\\right)^{T}\\beta\\left(\\mathbf{t}-\\mathbf{\\Phi w}\\right) \\right) = exp \\left( \\mathbf{w}^{T}\\left(\\mathbf{S_{0}}^{-1}+\\beta\\mathbf{\\Phi^{T}\\Phi}\\right)\\mathbf{w}-2\\mathbf{w}^{T}\\left(\\mathbf{S_{0}}^{-1}\\mathbf{m_0}+\\beta\\mathbf{\\Phi}^{T}\\mathbf{t}\\right)+const(\\mathbf{w})\\right)$$\n",
    "\n",
    "From this two expresions we can conclude that is the mean and covariance matrix: \n",
    "$$\\mathbf{S_{N}}^{-1}=\\left(\\mathbf{S_{0}}^{-1}+\\beta\\mathbf{\\Phi^{T}\\Phi}\\right) $$\n",
    "and \n",
    "$$\\mathbf{S_{N}}^{-1}\\mathbf{m_N}=\\left(\\mathbf{S_{0}}^{-1}\\mathbf{m_0}+\\beta\\mathbf{\\Phi}^{T}\\mathbf{t}\\right)$$\n",
    "$$\\mathbf{m_N}=\\mathbf{S_{N}}\\left(\\mathbf{S_{0}}^{-1}\\mathbf{m_0}+\\beta\\mathbf{\\Phi}^{T}\\mathbf{t}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. MLE for the offset term in linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the optimal $w_0$, we need to write down our log likelihood: \n",
    "$$LL= const(w_0)+\\frac{1}{2\\sigma^2}\\sum_{n=1}^N  \\left( w_0 +\\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x_n}) -\\mathbf{t_n}\\right)^2 $$\n",
    "$$\\frac{\\partial LL}{\\partial w_0} = \\frac{1}{\\sigma^2}\\sum_{n=1}^N  \\left( w_0 +\\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x_n}) -\\mathbf{t_n}\\right) = 0)$$\n",
    "$$w_0 = \\frac{1}{N}\\left(\\sum_{n=1}^{N}\\mathbf{t_n}- \\mathbf{w}^{T}\\mathbf{\\phi}(\\mathbf{x_n})\\right) = \\\\\n",
    "\\frac{1}{N}\\sum_{n=1}^{N}\\mathbf{t_n} -\\frac{1}{N}\\sum_{n=1}^{N} \\mathbf{w}^{T}\\mathbf{\\phi}(\\mathbf{x_n})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\partial LL}{\\partial \\mathbf{w}}= 0 = \\sum_{n = 1}^N (\\phi_n -\\bar{\\phi} ) (\\mathbf{t_n} -\\mathbf{\\bar{t}}  - \\mathbf{w}^T (\\mathbf{\\phi}(\\mathbf{x_n})-\\bar{\\phi}))\n",
    "\\end{equation}\n",
    "Using the best solution for $w_0$\n",
    "\\begin{equation}\n",
    "0 = \\Phi_c^T \\mathbf{t} - \\Phi_c^T \\bar{\\mathbf{t}} + \\Phi_c^T \\bar{\\Phi_c} \\mathbf{w}  - \\Phi_c^T \\Phi_c \\mathbf{w}\n",
    "\\end{equation}\n",
    "So \n",
    "$$\\mathbf{w}=(\\Phi_c^T \\Phi_c)^{-1} \\Phi_c^T \\mathbf{t_c}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
