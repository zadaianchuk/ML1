{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Maximum likelihood of classification with shared covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write donw loglikelihood using the fact that $t_{nk}$ is equal to 1 only if $\\phi_n \\sim \\mathcal{N}(\\mu_k,\\Sigma)$\n",
    "\\begin{equation}\n",
    "    LL =const(\\mu_1,...,\\mu_{K},\\Sigma) -\\frac{1}{2} \\sum_{n=1}^N\\sum_{k=1}^K t_{nk} \\left( \\ln |\\Sigma|+(\\phi_n-\\mu_k)^T \\Sigma^{-1}(\\phi_n-\\mu_k)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial LL}{\\partial\\mu_{l}} =\\sum_{n=1}^N\\sum_{k=1}^K t_{nk} \\frac{\\partial\\left( (\\phi_n-\\mu_k)^T \\Sigma^{-1}(\\phi_n-\\mu_k)\\right)}{\\partial\\mu_l}\n",
    "\\end{equation}\n",
    "\n",
    "$\\frac{\\partial\\left( (\\phi_n-\\mu_k)^T \\Sigma^{-1}(\\phi_n-\\mu_k)\\right)}{\\partial\\mu_l}$ is not equal to zero only for $k=l$\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial LL}{\\partial\\mu_{l}} =\\sum_{n=1}^Nt_{nl} \\frac{\\partial\\left((\\phi_n-\\mu_l)^T \\Sigma^{-1}(\\phi_n-\\mu_l)\\right)}{\\partial\\mu_l} = 0\n",
    "\\end{equation}\n",
    "\n",
    "From $\\frac{\\partial a^TBa}{\\partial a} = a^T(B+B^T)$ and $\\frac{\\partial a^Tb}{\\partial a}=\\frac{\\partial b^Ta}{\\partial a}=b$\n",
    "\\begin{equation}\n",
    "     \\sum_{n=1}^N t_{nl} \\left(\\Sigma^{-1}(\\phi_n-\\mu_l)\\right) = 0 \\\\\n",
    "   \\Sigma^{-1}\\sum_{n=1}^N t_{nl} \\left((\\phi_n-\\mu_l)\\right) = 0 \\\\\n",
    "   \\sum_{n=1}^N t_{nl} \\phi_n= \\sum_{n=1}^N t_{nl}\\mu_l =N_k \\mu_l \\\\\n",
    "   \\mu_l = \\frac{1}{N_k}\\sum_{n=1}^N t_{nl} \\phi_n\n",
    "\\end{equation}\n",
    "\n",
    "For finding maximum likelihood $\\Sigma$, we will differentiate w.r.t $\\Sigma^{-1}$ \n",
    "\\begin{equation}\n",
    "  \\frac{\\partial LL}{\\partial \\Sigma^{-1}} = 0 \\Leftrightarrow \\frac{\\partial LL}{\\partial \\Sigma} = 0\n",
    "\\end{equation}\n",
    "To do this we need to use several formulas from Bishop Appendix C: \n",
    "\\begin{equation}\n",
    "  \\frac{\\partial \\ln|A|}{A} =\\left(A^{-1}\\right)^T; \\\\\n",
    "\\end{equation}\n",
    "As $a^T Ba$ a scalar, then $a^T Ba=\\text{Tr}( a^T Ba)$,\n",
    "\n",
    "using $\\text{Tr}( ABC)= \\text{Tr}( BCA) = \\text{Tr}(CAB)$ we get that \n",
    "\\begin{equation}\n",
    "  a^T Ba = \\text{Tr}(Baa^T) \n",
    "\\end{equation}\n",
    "Now we can use Bishop Appendix C.25:\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial \\text{Tr}(B^T aa^T)}{\\partial B} = aa^T\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial LL}{\\partial \\Sigma^{-1}} = \\sum_{n=1}^N\\sum_{k=1}^K t_{nk} \\left( \\Sigma - \n",
    " (\\phi_n-\\mu_k) (\\phi_n-\\mu_k)^T\\right) = 0\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    " \\sum_{n=1}^N\\sum_{k=1}^K t_{nk} \\Sigma=  \\sum_{n=1}^N\\sum_{k=1}^K t_{nk} (\\phi_n-\\mu_k) (\\phi_n-\\mu_k)^T \\\\\n",
    " N \\Sigma=  \\sum_{n=1}^N\\sum_{k=1}^K t_{nk} (\\phi_n-\\mu_k) (\\phi_n-\\mu_k)^T \\\\\n",
    " \\Sigma= \\sum_{k=1}^K \\frac{N_k}{N}\\frac{1}{N_k} \\sum_{n=1}^N t_{nk} (\\phi_n-\\mu_k) (\\phi_n-\\mu_k)^T \\\\\n",
    " \\Sigma= \\sum_{k=1}^K \\frac{N_k}{N}  \\frac{1}{N_k} \\sum_{n=1}^N t_{nk} (\\phi_n-\\mu_k) (\\phi_n-\\mu_k)^T \\\\\n",
    " \\Sigma=  \\sum_{k=1}^K \\frac{N_k}{N} S_k\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Negative logarithm of likelihood for -1, +1 coding of targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Regularizing separate terms in 2d logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E5.\n",
    "\n",
    "$$w_0=0$$\n",
    "![E5](./fig/E5.png)\n",
    "\n",
    "E6.\n",
    "\n",
    "$$w_1=0$$\n",
    "![E5](./fig/E6.png)\n",
    "\n",
    "E7.\n",
    "\n",
    "$$w_2=0$$\n",
    "![E5](./fig/E7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Binary classification with mislabeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have some data $\\{x_i,\\tau_i\\}$ where as real classes are $t_i$. \n",
    "We know that the probability of mistake is $P(t_i \\neq \\tau_i)=\\varepsilon$. \n",
    "Let us write down the likelihood of $\\{x_i, t_i\\}$\n",
    "$$LL =  \\prod_{i=1}^N \\left( y_i^{\\tau_i}(1-y_i)^{1-\\tau_i}\\cdot(1-\\varepsilon)+y_i^{1-\\tau_i}(1-y_i)^{\\tau_i}\\cdot\\varepsilon\\right)$$\n",
    "$$Error(\\varepsilon) = - \\sum_{i=1}^N \\log \\left( y_i^{\\tau_i}(1-y_i)^{1-\\tau_i}\\cdot(1-\\varepsilon)+y_i^{1-\\tau_i}(1-y_i)^{\\tau_i}\\cdot\\varepsilon\\right)$$\n",
    "$$Error(0) = - \\sum_{i=1}^N \\log \\left( y_i^{\\tau_i}(1-y_i)^{1-\\tau_i}\\right) = - \\sum_{i=1}^N \\tau_i \\log y_i + 1-\\tau_i \\log (1-y_i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Gradient descent for quadratic error function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $\\frac{\\partial a^TBa}{\\partial a} = a^T(B+B^T)$ and $\\frac{\\partial a^Tb}{\\partial a}=\\frac{\\partial b^Ta}{\\partial a}=b$\n",
    "\\begin{equation}\n",
    " w^{\\tau} = w^{\\tau-1} - \\rho (w^{\\tau-1}-w^{*})H \\quad | \\cdot u_j\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    " \\omega_j^{\\tau} = \\omega_j^{\\tau-1} - \\rho (w^{\\tau-1}-w^{*})\\eta_j u_j \\\\\n",
    " \\omega_j^{\\tau} = \\omega_j^{\\tau-1} - \\rho \\eta_j (\\omega_j^{\\tau-1}-\\omega_j^{*}) \\\\\n",
    " \\omega_j^{\\tau} = \\omega_j^{\\tau-1}(1 - \\rho \\eta_j) + \\rho \\eta_j \\omega_j^{*} = ... = \\\\\n",
    " \\omega_j^{0}*(..)+(1+(1 - \\rho \\eta_j)+(1 - \\rho \\eta_j)^2+...+(1 - \\rho \\eta_j)^{\\tau-1})\\rho \\eta_j \\omega_j^{*}\n",
    "\\end{equation}\n",
    "\n",
    "As said $\\omega_j^{0} = 0 $ and using \"Difference of n-th powers\" formula: \n",
    "$a^n-b^n=(a-b)(a^{n-1}b+..+ab^{n-1})$ we get \n",
    "\\begin{equation}\n",
    " \\omega_j^{\\tau} =\\frac{(1-(1-\\rho \\eta_j)^n}{1-(1-\\rho \\eta_j)}\\rho \\eta_j\\omega_j^{*} = \\left(1-(1-\\rho \\eta_j)^n\\right) \\omega_j^{*} \n",
    " \\end{equation}\n",
    " $a^n \\rightarrow 0$ when |a|<1, so $\\omega_j^{\\tau} \\rightarrow \\omega_j^{*}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
