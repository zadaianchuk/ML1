{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Maximum likelihood of classification with shared covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write donw loglikelihood using the fact that $t_{nk}$ is equal to 1 only if $\\phi_n \\sim \\mathcal{N}(\\mu_k,\\Sigma)$\n",
    "\\begin{equation}\n",
    "    LL =const(\\mu_1,...,\\mu_{K},\\Sigma) -\\frac{1}{2} \\sum_{n=1}^N\\sum_{k=1}^K t_{nk} \\left( \\ln |\\Sigma|+(\\phi_n-\\mu_k)^T \\Sigma^{-1}(\\phi_n-\\mu_k)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial LL}{\\partial\\mu_{l}} =\\sum_{n=1}^N\\sum_{k=1}^K t_{nk} \\frac{\\partial\\left( (\\phi_n-\\mu_k)^T \\Sigma^{-1}(\\phi_n-\\mu_k)\\right)}{\\partial\\mu_l}\n",
    "\\end{equation}\n",
    "\n",
    "$\\frac{\\partial\\left( (\\phi_n-\\mu_k)^T \\Sigma^{-1}(\\phi_n-\\mu_k)\\right)}{\\partial\\mu_l}$ is not equal to zero only for $k=l$\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial LL}{\\partial\\mu_{l}} =\\sum_{n=1}^Nt_{nl} \\frac{\\partial\\left((\\phi_n-\\mu_l)^T \\Sigma^{-1}(\\phi_n-\\mu_l)\\right)}{\\partial\\mu_l} = 0\n",
    "\\end{equation}\n",
    "\n",
    "From $\\frac{\\partial a^TBa}{\\partial a} = (B+B^T)a$ and $\\frac{\\partial a^Tb}{\\partial a}=\\frac{\\partial b^Ta}{\\partial a}=b$\n",
    "\\begin{equation}\n",
    "     \\sum_{n=1}^N t_{nl} \\left(\\Sigma^{-1}(\\phi_n-\\mu_l)\\right) = 0 \\\\\n",
    "   \\Sigma^{-1}\\sum_{n=1}^N t_{nl} \\left((\\phi_n-\\mu_l)\\right) = 0 \\\\\n",
    "   \\sum_{n=1}^N t_{nl} \\phi_n= \\sum_{n=1}^N t_{nl}\\mu_l =N_k \\mu_l \\\\\n",
    "   \\mu_l = \\frac{1}{N_k}\\sum_{n=1}^N t_{nl} \\phi_n\n",
    "\\end{equation}\n",
    "\n",
    "For finding maximum likelihood $\\Sigma$, we will differentiate w.r.t $\\Sigma^{-1}$ \n",
    "\\begin{equation}\n",
    "  \\frac{\\partial LL}{\\partial \\Sigma^{-1}} = 0 \n",
    "\\end{equation}\n",
    "To do this we need to use several formulas from Bishop Appendix C: \n",
    "\\begin{equation}\n",
    "  \\frac{\\partial \\ln|A|}{\\partial A} =\\left(A^{-1}\\right)^T; \\\\\n",
    "\\end{equation}\n",
    "In our case we have \n",
    "\\begin{equation}\n",
    "  \\frac{\\partial \\ln|\\Sigma|}{\\partial\\Sigma^{-1}} =\\frac{\\partial \\ln\\frac{1}{|\\Sigma^{-1}|}}{\\partial\\Sigma^{-1}}=- \\frac{\\partial \\ln|\\Sigma^{-1}|}{\\partial\\Sigma^{-1}}=\\left({\\Sigma^{-1}}^{-1}\\right)^T = -\\Sigma; \\\\\n",
    "\\end{equation}\n",
    "As $a^T Ba$ a scalar, then $a^T Ba=\\text{Tr}( a^T Ba)$,\n",
    "\n",
    "using $\\text{Tr}( ABC)= \\text{Tr}( BCA) = \\text{Tr}(CAB)$ we get that \n",
    "\\begin{equation}\n",
    "  a^T Ba = \\text{Tr}(Baa^T) \n",
    "\\end{equation}\n",
    "Now we can use Bishop Appendix C.25:\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial \\text{Tr}(B^T aa^T)}{\\partial B} = aa^T\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial LL}{\\partial \\Sigma^{-1}} = \\sum_{n=1}^N\\sum_{k=1}^K t_{nk} \\left( \\Sigma - \n",
    " (\\phi_n-\\mu_k) (\\phi_n-\\mu_k)^T\\right) = 0\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    " \\sum_{n=1}^N\\sum_{k=1}^K t_{nk} \\Sigma=  \\sum_{n=1}^N\\sum_{k=1}^K t_{nk} (\\phi_n-\\mu_k) (\\phi_n-\\mu_k)^T \\\\\n",
    " N \\Sigma=  \\sum_{n=1}^N\\sum_{k=1}^K t_{nk} (\\phi_n-\\mu_k) (\\phi_n-\\mu_k)^T \\\\\n",
    " \\Sigma= \\sum_{k=1}^K \\frac{N_k}{N}\\frac{1}{N_k} \\sum_{n=1}^N t_{nk} (\\phi_n-\\mu_k) (\\phi_n-\\mu_k)^T \\\\\n",
    " \\Sigma= \\sum_{k=1}^K \\frac{N_k}{N}  \\frac{1}{N_k} \\sum_{n=1}^N t_{nk} (\\phi_n-\\mu_k) (\\phi_n-\\mu_k)^T \\\\\n",
    " \\Sigma=  \\sum_{k=1}^K \\frac{N_k}{N} S_k\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Negative logarithm of likelihood for -1, +1 coding of targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was explained in the hint the probability of the different events is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "P(t_n|x_n) = \\sigma(t_n y_n) \n",
    "\\end{equation}\n",
    "with $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$.\n",
    "\n",
    "\n",
    "Our likelihood is product of this probabilities for all data points: \n",
    "\n",
    "\\begin{equation}\n",
    "L = \\prod\\limits_{n=1}^N \\sigma(t_n y_n)\n",
    "\\end{equation}\n",
    "\n",
    "So the negative loglikelihood is equal to\n",
    "\\begin{equation}\n",
    "Eroor=-LL = \\sum\\limits_{n=1}^N \\log (1+e^{- t_n y_n})\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Regularizing separate terms in 2d logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E5.\n",
    "\n",
    "$$w_0=0$$\n",
    "![E5](./fig/E5.png)\n",
    "\n",
    "E6.\n",
    "\n",
    "$$w_1=0$$\n",
    "![E5](./fig/E6.png)\n",
    "\n",
    "E7.\n",
    "\n",
    "$$w_2=0$$\n",
    "![E5](./fig/E7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Binary classification with mislabeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have some data $\\{x_i,\\tau_i\\}_{i=1}^N$ where as real classes are $t_i$. \n",
    "We know that the probability of mistake is $P(t_i \\neq \\tau_i)=\\varepsilon$. \n",
    "Let us write down the likelihood of $\\{x_i, t_i\\}$ : \n",
    "$$P(t_i|x_i)=P(\\tau_i|x_i)\\cdot P(\\tau_i=t_i)+P(1-\\tau_i|x_i)\\cdot P(\\tau_i\\neq t_i)$$\n",
    "\n",
    "Now we can write down the likelihood for our data $\\{x_i,\\tau_i\\}_{i=1}^N$:  \n",
    "$$L =  \\prod_{i=1}^N \\left( y_i^{\\tau_i}(1-y_i)^{1-\\tau_i}\\cdot(1-\\varepsilon)+y_i^{1-\\tau_i}(1-y_i)^{\\tau_i}\\cdot\\varepsilon\\right)$$\n",
    "The error or loss function is equal to the negative loglikelihood:\n",
    "$$Error(\\varepsilon) = - \\sum_{i=1}^N \\log \\left( y_i^{\\tau_i}(1-y_i)^{1-\\tau_i}\\cdot(1-\\varepsilon)+y_i^{1-\\tau_i}(1-y_i)^{\\tau_i}\\cdot\\varepsilon\\right)$$\n",
    "$$Error(0) = - \\sum_{i=1}^N \\log \\left( y_i^{\\tau_i}(1-y_i)^{1-\\tau_i}\\right) = - \\sum_{i=1}^N \\tau_i \\log y_i + (1-\\tau_i) \\log (1-y_i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Gradient descent for quadratic error function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $\\frac{\\partial a^TBa}{\\partial a} = (B+B^T)a$ and $\\frac{\\partial a^Tb}{\\partial a}=\\frac{\\partial b^Ta}{\\partial a}=b$\n",
    "\n",
    "Next we want to use $Hu_j = \\eta_j u_j$. Let us transpose both sides of the equation $u_j^TH^T = \\eta_j u_j^T$ and use the $H=H^T$. \n",
    "\n",
    "For the projections we also can use $\\omega_j^{\\tau}={w^{\\tau}}^T u_j=({w^{\\tau}}^T u_j)^T=u_j^T w^{\\tau}$.\n",
    "\n",
    "\\begin{equation}\n",
    " w^{\\tau} = w^{\\tau-1} - \\rho H(w^{\\tau-1}-w^{*}) \\quad |  u_j^T \\cdot \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    " \\omega_j^{\\tau} = \\omega_j^{\\tau-1} - \\rho \\eta_j u_j^T (w^{\\tau-1}-w^{*}) \\\\\n",
    " \\omega_j^{\\tau} = \\omega_j^{\\tau-1} - \\rho \\eta_j (\\omega_j^{\\tau-1}-\\omega_j^{*}) \\\\\n",
    " \\omega_j^{\\tau} = \\omega_j^{\\tau-1}(1 - \\rho \\eta_j) + \\rho \\eta_j \\omega_j^{*} = ... = \\\\\n",
    " \\omega_j^{0}*(..)+(1+(1 - \\rho \\eta_j)+(1 - \\rho \\eta_j)^2+...+(1 - \\rho \\eta_j)^{\\tau-1})\\rho \\eta_j \\omega_j^{*}\n",
    "\\end{equation}\n",
    "\n",
    "As said $\\omega_j^{0} = 0 $ and using \"Difference of n-th powers\" formula: \n",
    "$a^n-b^n=(a-b)(a^{n-1}b+..+ab^{n-1})$ we get \n",
    "\\begin{equation}\n",
    " \\omega_j^{\\tau} =\\frac{(1-(1-\\rho \\eta_j)^n}{1-(1-\\rho \\eta_j)}\\rho \\eta_j\\omega_j^{*} = \\left(1-(1-\\rho \\eta_j)^n\\right) \\omega_j^{*} \n",
    " \\end{equation}\n",
    " $a^n \\rightarrow 0$ when |a|<1, so $\\omega_j^{\\tau} \\rightarrow \\omega_j^{*}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
